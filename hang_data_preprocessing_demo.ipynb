{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/92/pwtffhb94b3526gcw6x7zlqw0000gn/T/ipykernel_55261/1709349325.py:2: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  noteevents = pd.read_csv('demo_data/NOTEEVENTS.csv.gz', compression = 'gzip')\n"
     ]
    }
   ],
   "source": [
    "admissions = pd.read_csv('demo_data/ADMISSIONS.csv.gz', compression = 'gzip')\n",
    "noteevents = pd.read_csv('demo_data/NOTEEVENTS.csv.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/92/pwtffhb94b3526gcw6x7zlqw0000gn/T/ipykernel_55261/1421443384.py:17: FutureWarning: DataFrameGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use DataFrame.fillna instead\n",
      "  admissions[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = admissions.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')\n",
      "/var/folders/92/pwtffhb94b3526gcw6x7zlqw0000gn/T/ipykernel_55261/1421443384.py:17: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  admissions[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = admissions.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')\n",
      "/var/folders/92/pwtffhb94b3526gcw6x7zlqw0000gn/T/ipykernel_55261/1421443384.py:17: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  admissions[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = admissions.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')\n"
     ]
    }
   ],
   "source": [
    "admissions.ADMITTIME = pd.to_datetime(admissions.ADMITTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "admissions.DISCHTIME = pd.to_datetime(admissions.DISCHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "admissions.DEATHTIME = pd.to_datetime(admissions.DEATHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "admissions = admissions.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "admissions = admissions.reset_index(drop = True)\n",
    "admissions['NEXT_ADMITTIME'] = admissions.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
    "admissions['NEXT_ADMISSION_TYPE'] = admissions.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)\n",
    "\n",
    "rows = admissions.NEXT_ADMISSION_TYPE == 'ELECTIVE'\n",
    "admissions.loc[rows,'NEXT_ADMITTIME'] = pd.NaT\n",
    "admissions.loc[rows,'NEXT_ADMISSION_TYPE'] = np.NaN\n",
    "\n",
    "admissions = admissions.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "\n",
    "#When we filter out the \"ELECTIVE\", we need to correct the next admit time for these admissions since there might be 'emergency' next admit after \"ELECTIVE\"\n",
    "admissions[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = admissions.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')\n",
    "admissions['DAYS_NEXT_ADMIT']=  (admissions.NEXT_ADMITTIME - admissions.DISCHTIME).dt.total_seconds()/(24*60*60)\n",
    "admissions['OUTPUT_LABEL'] = (admissions.DAYS_NEXT_ADMIT < 30).astype('int')\n",
    "### filter out newborn and death\n",
    "admissions = admissions[admissions['ADMISSION_TYPE']!='NEWBORN']\n",
    "admissions = admissions[admissions.DEATHTIME.isnull()]\n",
    "admissions['DURATION'] = (admissions['DISCHTIME']-admissions['ADMITTIME']).dt.total_seconds()/(24*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "noteevents = noteevents.sort_values(by=['SUBJECT_ID','HADM_ID','CHARTDATE'])\n",
    "adm_notes = pd.merge(admissions[['SUBJECT_ID','HADM_ID','ADMITTIME','DISCHTIME','DAYS_NEXT_ADMIT','NEXT_ADMITTIME','ADMISSION_TYPE','DEATHTIME','OUTPUT_LABEL','DURATION']],\n",
    "                     noteevents[['SUBJECT_ID','HADM_ID','CHARTDATE','TEXT','CATEGORY']], \n",
    "                     on = ['SUBJECT_ID','HADM_ID'],\n",
    "                     how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_notes_copy = adm_notes.copy()\n",
    "adm_notes = adm_notes.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/92/pwtffhb94b3526gcw6x7zlqw0000gn/T/ipykernel_55261/2455160033.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  adm_notes.ADMITTIME_C = adm_notes.ADMITTIME.apply(lambda x: str(x).split(' ')[0])\n"
     ]
    }
   ],
   "source": [
    "adm_notes.ADMITTIME_C = adm_notes.ADMITTIME.apply(lambda x: str(x).split(' ')[0])\n",
    "adm_notes['ADMITTIME_C'] = pd.to_datetime(adm_notes.ADMITTIME_C, format = '%Y-%m-%d', errors = 'coerce')\n",
    "adm_notes['CHARTDATE'] = pd.to_datetime(adm_notes.CHARTDATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "\n",
    "### If Discharge Summary \n",
    "discharge = adm_notes[adm_notes['CATEGORY'] == 'Discharge summary']\n",
    "# multiple discharge summary for one admission -> after examination -> replicated summary -> replace with the last one \n",
    "discharge = (discharge.groupby(['SUBJECT_ID','HADM_ID']).nth(-1)).reset_index()\n",
    "discharge = discharge[discharge['TEXT'].notnull()]\n",
    "\n",
    "### If Less than n days on admission notes (Early notes)\n",
    "def less_n_days_data (adm_notes, n):\n",
    "    \n",
    "    less_n = adm_notes[((adm_notes['CHARTDATE']-adm_notes['ADMITTIME_C']).dt.total_seconds()/(24*60*60))<n]\n",
    "    less_n=less_n[less_n['TEXT'].notnull()]\n",
    "    #concatenate first\n",
    "    concat = pd.DataFrame(less_n.groupby('HADM_ID')['TEXT'].apply(lambda x: \"%s\" % ' '.join(x))).reset_index()\n",
    "    concat['OUTPUT_LABEL'] = concat['HADM_ID'].apply(lambda x: less_n[less_n['HADM_ID']==x].OUTPUT_LABEL.values[0])\n",
    "\n",
    "    return concat\n",
    "\n",
    "less_2 = less_n_days_data(adm_notes, 2)\n",
    "less_3 = less_n_days_data(adm_notes, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess1(x):\n",
    "    y=re.sub('\\\\[(.*?)\\\\]','',x) #remove de-identified brackets\n",
    "    y=re.sub('[0-9]+\\.','',y) #remove 1.2. since the segmenter segments based on this\n",
    "    y=re.sub('dr\\.','doctor',y)\n",
    "    y=re.sub('m\\.d\\.','md',y)\n",
    "    y=re.sub('admission date:','',y)\n",
    "    y=re.sub('discharge date:','',y)\n",
    "    y=re.sub('--|__|==','',y)\n",
    "    return y\n",
    "\n",
    "def preprocessing(df_less_n): \n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].fillna(' ')\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].str.replace('\\n',' ')\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].str.replace('\\r',' ')\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].apply(str.strip)\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].str.lower()\n",
    "\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].apply(lambda x: preprocess1(x))\n",
    "\n",
    "    df_len = len(df_less_n)\n",
    "    want=pd.DataFrame({'ID':[],'TEXT':[],'Label':[]})\n",
    "    for i in tqdm(range(df_len)):\n",
    "        x=df_less_n.TEXT.iloc[i].split()\n",
    "        n=int(len(x)/318)\n",
    "        for j in range(n):\n",
    "            new_row = pd.DataFrame({'TEXT':' '.join(x[j*318:(j+1)*318]),'Label':df_less_n.OUTPUT_LABEL.iloc[i],'ID':df_less_n.HADM_ID.iloc[i]}, index=[0])\n",
    "            want=pd.concat([want, new_row],ignore_index=True)\n",
    "        if len(x)%318>10:\n",
    "            new_row = pd.DataFrame({'TEXT':' '.join(x[-(len(x)%318):]),'Label':df_less_n.OUTPUT_LABEL.iloc[i],'ID':df_less_n.HADM_ID.iloc[i]}, index=[0])\n",
    "            want=pd.concat([want, new_row],ignore_index=True)\n",
    "    \n",
    "    return want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:00<00:00, 355.27it/s]\n",
      "100%|██████████| 297/297 [00:00<00:00, 1525.64it/s]\n",
      "100%|██████████| 392/392 [00:00<00:00, 1573.59it/s]\n"
     ]
    }
   ],
   "source": [
    "discharge = preprocessing(discharge)\n",
    "less_2 = preprocessing(less_2)\n",
    "less_3 = preprocessing(less_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date of birth: sex: f service: medicine allergies: methadone attending: chief complaint: neck pain, feeling \"awful\" major surgical or invasive procedure: placement of triple lumen catheter in right ij history of present illness: 56 year old woman with dm, htn, esrd on hd, recent mva, who presents with neck pain and feeling \"awful.\" she was involved in a mva two weeks ago when another car struck her car from the passenger side. she was in the passenger seat, wearing a seatbelt, did not strike her head but did experience a jerking motion of her neck. afterward she began to experience severe headaches and posterior neck pain. she was seen at on and diagnosed with whiplash. x-rays were ordered which she did not have done. she went to hospital yesterday for the pain and was given dilaudid and ativan and discharged home. she continued to feel unwell so came to the ed. in the ed she initially triggered for hypoxia of 82% on ra in triage, but on recheck was 100% on ra. other vs were 0, 47, 94/35, she was a&ox3 but slightly lethargic with pain over her posterior c-spine. ekg with a junctional rhythm (old) and no ischemic changes. she dropped her sbp to the 80s so a rij was placed and she was given a dose of empiric zosyn and 2l ns. she did not required pressors. notable for hct 25 (baseline high 20s-low 30s), stool guiac neg. cxr with mild vascular congestion but no pneumonia or other acute process. vs prior to transfer were 98/50 (map 63), 46, satting in the high 90s on 4l. on arrival to the micu, the patient continues to experience severe posterior neck pain and headache. having chest pain in the center of her chest, gerd-like, feels like she needs to burp, worse with deep breaths, and then vomited twice (2nd time dark, guiac positive). her symptoms resolved with anti-emetics,'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discharge['TEXT'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "readmit_ID = admissions[admissions.OUTPUT_LABEL == 1].HADM_ID\n",
    "not_readmit_ID = admissions[admissions.OUTPUT_LABEL == 0].HADM_ID\n",
    "#subsampling to get the balanced pos/neg numbers of patients for each dataset\n",
    "not_readmit_ID_use = not_readmit_ID.sample(n=len(readmit_ID), random_state=1)\n",
    "id_val_test_t=readmit_ID.sample(frac=0.2,random_state=1)\n",
    "id_val_test_f=not_readmit_ID_use.sample(frac=0.2,random_state=1)\n",
    "\n",
    "id_train_t = readmit_ID.drop(id_val_test_t.index)\n",
    "id_train_f = not_readmit_ID_use.drop(id_val_test_f.index)\n",
    "\n",
    "id_val_t=id_val_test_t.sample(frac=0.5,random_state=1)\n",
    "id_test_t=id_val_test_t.drop(id_val_t.index)\n",
    "\n",
    "id_val_f=id_val_test_f.sample(frac=0.5,random_state=1)\n",
    "id_test_f=id_val_test_f.drop(id_val_f.index)\n",
    "\n",
    "# test if there is overlap between train and test, should return \"array([], dtype=int64)\"\n",
    "(pd.Index(id_test_t).intersection(pd.Index(id_train_t))).values\n",
    "\n",
    "id_test = pd.concat([id_test_t, id_test_f])\n",
    "test_id_label = pd.DataFrame(data = list(zip(id_test, [1]*len(id_test_t)+[0]*len(id_test_f))), columns = ['id','label'])\n",
    "\n",
    "id_val = pd.concat([id_val_t, id_val_f])\n",
    "val_id_label = pd.DataFrame(data = list(zip(id_val, [1]*len(id_val_t)+[0]*len(id_val_f))), columns = ['id','label'])\n",
    "\n",
    "id_train = pd.concat([id_train_t, id_train_f])\n",
    "train_id_label = pd.DataFrame(data = list(zip(id_train, [1]*len(id_train_t)+[0]*len(id_train_f))), columns = ['id','label'])\n",
    "\n",
    "#get discharge train/val/test\n",
    "\n",
    "discharge_train = discharge[discharge.ID.isin(train_id_label.id)]\n",
    "discharge_val = discharge[discharge.ID.isin(val_id_label.id)]\n",
    "discharge_test = discharge[discharge.ID.isin(test_id_label.id)]\n",
    "\n",
    "# subsampling for training....since we obtain training on patient admission level so now we have same number of pos/neg readmission\n",
    "# but each admission is associated with different length of notes and we train on each chunks of notes, not on the admission, we need\n",
    "# to balance the pos/neg chunks on training set. (val and test set are fine) Usually, positive admissions have longer notes, so we need \n",
    "# find some negative chunks of notes from not_readmit_ID that we haven't used yet\n",
    "\n",
    "df = pd.concat([not_readmit_ID_use, not_readmit_ID])\n",
    "df = df.drop_duplicates(keep=False)\n",
    "#check to see if there are overlaps\n",
    "(pd.Index(df).intersection(pd.Index(not_readmit_ID_use))).values\n",
    "\n",
    "# for this set of split with random_state=1, we find we need 400 more negative training samples\n",
    "not_readmit_ID_more = df.sample(n=400, random_state=1)\n",
    "discharge_train_snippets = pd.concat([discharge[discharge.ID.isin(not_readmit_ID_more)], discharge_train])\n",
    "\n",
    "#shuffle\n",
    "discharge_train_snippets = discharge_train_snippets.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "#check if balanced\n",
    "discharge_train_snippets.Label.value_counts()\n",
    "\n",
    "discharge_train_snippets.to_csv('./discharge_train.csv')\n",
    "discharge_val.to_csv('./discharge_val.csv')\n",
    "discharge_test.to_csv('./discharge_test.csv')\n",
    "\n",
    "### for Early notes experiment: we only need to find training set for 3 days, then we can test \n",
    "### both 3 days and 2 days. Since we split the data on patient level and experiments share admissions\n",
    "### in order to see the progression, the 2 days training dataset is a subset of 3 days training set.\n",
    "### So we only train 3 days and we can test/val on both 2 & 3days or any time smaller than 3 days. This means\n",
    "### if we train on a dataset with all the notes in n days, we can predict readmissions smaller than n days. \n",
    "\n",
    "#for 3 days note, similar to discharge\n",
    "\n",
    "early_train = less_3[less_3.ID.isin(train_id_label.id)]\n",
    "not_readmit_ID_more = df.sample(n=500, random_state=1)\n",
    "early_train_snippets = pd.concat([less_3[less_3.ID.isin(not_readmit_ID_more)], early_train])\n",
    "#shuffle\n",
    "early_train_snippets = early_train_snippets.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "early_train_snippets.to_csv('./3days_train.csv')\n",
    "\n",
    "early_val = less_3[less_3.ID.isin(val_id_label.id)]\n",
    "early_val.to_csv('./3days_val.csv')\n",
    "\n",
    "# we want to test on admissions that are not discharged already. So for less than 3 days of notes experiment,\n",
    "# we filter out admissions discharged within 3 days\n",
    "actionable_ID_3days = admissions[admissions['DURATION'] >= 3].HADM_ID\n",
    "test_actionable_id_label = test_id_label[test_id_label.id.isin(actionable_ID_3days)]\n",
    "early_test = less_3[less_3.ID.isin(test_actionable_id_label.id)]\n",
    "\n",
    "early_test.to_csv('./3days_test.csv')\n",
    "\n",
    "#for 2 days notes, we only obtain test set. Since the model parameters are tuned on the val set of 3 days\n",
    "\n",
    "actionable_ID_2days = admissions[admissions['DURATION'] >= 2].HADM_ID\n",
    "\n",
    "test_actionable_id_label_2days = test_id_label[test_id_label.id.isin(actionable_ID_2days)]\n",
    "\n",
    "early_test_2days = less_2[less_2.ID.isin(test_actionable_id_label_2days.id)]\n",
    "\n",
    "early_test_2days.to_csv('./2days_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
